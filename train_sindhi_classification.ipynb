{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajavavek/DAugSindhi/blob/main/train_sindhi_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "5g2IXiIhQqjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jd1rts6RJbUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "N484YhDhf7C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkh0eYkEItsY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def swap_words(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words) < 2:\n",
        "        return sentence\n",
        "    idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def rs(df):\n",
        "  # Augment the dataframe by swapping words in each row\n",
        "  augmented_df = df.copy()\n",
        "  augmented_df['text'] = augmented_df['text'].apply(swap_words)\n",
        "  return augmented_df\n",
        "\n",
        "def remove_word(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words) < 2:\n",
        "        return sentence\n",
        "    word_to_remove = random.choice(words)\n",
        "    words.remove(word_to_remove)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def rd(df):\n",
        "  # Augment the dataframe by swapping words in each row\n",
        "  augmented_df = df.copy()\n",
        "  augmented_df['text'] = augmented_df['text'].apply(remove_word)\n",
        "  return augmented_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def llm_augment_text(prompt, num_augmentations=1, length=100, temperature=0.8):\n",
        "    # Load the GPT-2 model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text augmentations\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        do_sample=True,\n",
        "        max_length=length,\n",
        "        num_return_sequences=num_augmentations,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    # Decode the generated output into text\n",
        "    augmentations = []\n",
        "    for text in output:\n",
        "        decoded_text = tokenizer.decode(text, skip_special_tokens=True)\n",
        "        augmentations.append(decoded_text)\n",
        "\n",
        "    return augmentations[0]\n",
        "\n",
        "def llm_expand(df):\n",
        "  # Augment the dataframe by swapping words in each row\n",
        "  augmented_df = df.copy()\n",
        "  augmented_df['text'] = augmented_df['text'].apply(llm_augment_text)\n",
        "  return augmented_df"
      ],
      "metadata": {
        "id": "sxaHjJZmbXfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "excel_file = pd.ExcelFile('/content/drive/MyDrive/256_PROJECT/256_input.xlsx')\n",
        "\n",
        "# Create an empty dictionary to store the dataframes\n",
        "dfs = {}\n",
        "\n",
        "# Iterate over each sheet in the Excel file\n",
        "for sheet_name in excel_file.sheet_names:\n",
        "    # Read the sheet as a dataframe\n",
        "    df = excel_file.parse(sheet_name)\n",
        "    # Store the dataframe in the dictionary\n",
        "    dfs[sheet_name] = df\n",
        "\n",
        "# Access the dataframes by sheet name\n",
        "for sheet_name, df in dfs.items():\n",
        "    print(f\"Sheet Name: {sheet_name}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "s-UM8c3lJ0F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del dfs[\"Urdu - 3 class(Train)\"]\n",
        "del dfs[\"Urdu - 3 class(Test)\"]"
      ],
      "metadata": {
        "id": "-5AmXc0SOfFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sheet_name, df in dfs.items():\n",
        "    print(f\"Sheet Name: {sheet_name}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "loy7RUZMOrig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess(train_name, test_name):\n",
        "  train = dfs[train_name]\n",
        "  test = dfs[test_name]\n",
        "\n",
        "  train = train.dropna()\n",
        "  test = test.dropna()\n",
        "\n",
        "  train.reset_index(drop=True, inplace=True)\n",
        "  test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "\n",
        "  train['label_encoded'] = label_encoder.fit_transform(train['label'])\n",
        "  test['label_encoded'] = label_encoder.fit_transform(test['label'])\n",
        "\n",
        "  train.drop('label', axis=1, inplace=True)\n",
        "  test.drop('label', axis=1, inplace=True)\n",
        "\n",
        "  train.rename(columns={'label_encoded': 'label'}, inplace=True)\n",
        "  test.rename(columns={'label_encoded': 'label'}, inplace=True)\n",
        "\n",
        "  train['label'] = train['label'].astype(str)\n",
        "  test['label'] = test['label'].astype(str)\n",
        "\n",
        "  train['text'] = train['text'].astype(str)\n",
        "  test['text'] = test['text'].astype(str)\n",
        "\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "YRN2gbLBQFnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def calculate_metrics(actual, predicted):\n",
        "    accuracy = accuracy_score(actual, predicted)\n",
        "    precision = precision_score(actual, predicted, average='macro')\n",
        "    recall = recall_score(actual, predicted, average='macro')\n",
        "    f1_macro = f1_score(actual, predicted, average='macro')\n",
        "    f1_micro = f1_score(actual, predicted, average='micro')\n",
        "\n",
        "    return(accuracy, precision, recall, f1_macro, f1_micro)"
      ],
      "metadata": {
        "id": "a1-P5p-ITPTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "def do_it(train_name, test_name):\n",
        "  try:\n",
        "    from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "    train, test = preprocess(train_name, test_name)\n",
        "\n",
        "    model_args = ClassificationArgs()\n",
        "    model_args.num_train_epochs = 3\n",
        "    model_args.train_batch_size = 16\n",
        "    model_args.eval_batch_size = 32\n",
        "    model_args.labels_list = list(train[\"label\"].unique())\n",
        "    model_args.max_seq_length = 512\n",
        "    model_args.overwrite_output_dir = True\n",
        "    # Add more configuration options as needed\n",
        "\n",
        "    model = ClassificationModel('bert', 'bert-base-multilingual-cased', num_labels = len(train[\"label\"].unique()), args=model_args, use_cuda=True)\n",
        "    model.train_model(train, eval_df=test)\n",
        "    predictions, raw_outputs = model.predict(test[\"text\"].values.tolist())\n",
        "    print(\"\\n\\n================================\\n\")\n",
        "    print(f\"W/O: {calculate_metrics(test['label'].values.tolist(), predictions)}\")\n",
        "\n",
        "    aug_df = rs(train)\n",
        "    train = train.append(aug_df)\n",
        "    model = ClassificationModel('bert', 'bert-base-multilingual-cased', num_labels = len(train[\"label\"].unique()), args=model_args, use_cuda=True)\n",
        "    model.train_model(train, eval_df=test)\n",
        "    predictions, raw_outputs = model.predict(test[\"text\"].values.tolist())\n",
        "    print(\"\\n\\n================================\\n\")\n",
        "    print(f\"W RS: {calculate_metrics(test['label'].values.tolist(), predictions)}\")\n",
        "\n",
        "    aug_df = rd(train)\n",
        "    train = train.append(aug_df)\n",
        "    model = ClassificationModel('bert', 'bert-base-multilingual-cased', num_labels = len(train[\"label\"].unique()), args=model_args, use_cuda=True)\n",
        "    model.train_model(train, eval_df=test)\n",
        "    predictions, raw_outputs = model.predict(test[\"text\"].values.tolist())\n",
        "    print(\"\\n\\n================================\\n\")\n",
        "    print(f\"W RD: {calculate_metrics(test['label'].values.tolist(), predictions)}\")\n",
        "\n",
        "    aug_df = llm_expand(train)\n",
        "    train = train.append(aug_df)\n",
        "    model = ClassificationModel('bert', 'bert-base-multilingual-cased', num_labels = len(train[\"label\"].unique()), args=model_args, use_cuda=True)\n",
        "    model.train_model(train, eval_df=test)\n",
        "    predictions, raw_outputs = model.predict(test[\"text\"].values.tolist())\n",
        "    print(\"\\n\\n================================\\n\")\n",
        "    print(f\"W LLM Expand: {calculate_metrics(test['label'].values.tolist(), predictions)}\")\n",
        "\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "rSap6AWdL0Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_it(\"Sindhi - 3 Class (Train)\", \"Sindhi - 3 Class (Test)\")"
      ],
      "metadata": {
        "id": "cF3RYrzDhHdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_it(\"Sindhi - 2 Class (Train)\", \"Sindhi - 2 Class (Train)\")"
      ],
      "metadata": {
        "id": "hdzWXAZ_hJWe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}